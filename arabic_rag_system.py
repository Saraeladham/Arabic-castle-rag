# -*- coding: utf-8 -*-
"""Arabic RAG system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1frQYtKyIEOgcxdmjEm4--twRvbqeoukI
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile arabic_rag.py

!pip install sentence-transformers scikit-learn

from google.colab import files
uploaded = files.upload()

with open("arabic.txt", "r", encoding="utf-8") as f:
    text = f.read()

print("‚úÖ File loaded successfully!")
print("üìÑ Preview (first 300 chars):\n")
print(text[:300])

# Chunking Strategy (Paragraph-based)

chunks = text.split("\n\n") # Split by blank lines = paragraph breaks
chunks = [chunk.strip() for chunk in chunks if chunk.strip()]  # Remove empty/whitespace chunks

print(f"\n Total number of chunks created: {len(chunks)}")
print(f"\n First chunk preview:\n{chunks[0][:300]}")

# Load Embedding Model (BAAI/bge-m3)

from sentence_transformers import SentenceTransformer

print("\n‚è≥ Loading BAAI/bge-m3 model (first time takes ~1 min to download)...")
model = SentenceTransformer("BAAI/bge-m3")
print("Model loaded successfully!")

# Embed all chunks into vectors
print("\n‚è≥ Embedding all chunks (converting text ‚Üí vectors)...")
chunk_embeddings = model.encode(chunks, show_progress_bar=True)

print(f"\n‚úÖ All chunks embedded!")
print(f"üìê Embedding shape: {chunk_embeddings.shape}")

# Helper: Split Arabic text into sentences
import re

def split_into_sentences(text):
    """
    Split Arabic (or mixed) text into sentences.
    Splits on Arabic/Latin sentence-ending punctuation.
    """
    sentences = re.split(r'(?<=[.!?ÿüÿå])\s+', text.strip())
    return [s.strip() for s in sentences if s.strip()]

# RAG Query Function
# top_k=3 is appropriate for a single-page doc:
#   - Captures enough relevant context
#   - Avoids returning the whole document
#   - Small corpus means chunks are few; k=3 covers ~25-50% safely
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def ask_question(question, chunks, chunk_embeddings, model, top_k=3):
    """
    Retrieve top_k most relevant chunks, then extract the single
    best sentence from them as a concise answer.

    Returns:
    - concise_answer : best matching sentence (short, direct)
    - source_chunk   : the full chunk the sentence came from
    - chunk_index    : index of that chunk in the original list
    - sentence_score : similarity score of the best sentence
    - chunk_score    : similarity score of the source chunk
    """

    # 1. Embed the question
    question_embedding = model.encode([question])

    # 2. Score all chunks and pick top_k
    chunk_scores = cosine_similarity(question_embedding, chunk_embeddings)[0]
    top_k_indices = np.argsort(chunk_scores)[::-1][:top_k]

    # 3. Collect all sentences from the top_k chunks + their embeddings
    candidate_sentences = []
    sentence_to_chunk_index = []  # track which chunk each sentence came from

    for idx in top_k_indices:
        sentences = split_into_sentences(chunks[idx])
        for sent in sentences:
            candidate_sentences.append(sent)
            sentence_to_chunk_index.append(idx)

    # 4. Re-rank sentences by similarity to the question
    if not candidate_sentences:
        return None

    sentence_embeddings = model.encode(candidate_sentences)
    sentence_scores = cosine_similarity(question_embedding, sentence_embeddings)[0]
    best_sent_idx = np.argmax(sentence_scores)

    best_sentence = candidate_sentences[best_sent_idx]
    source_chunk_index = sentence_to_chunk_index[best_sent_idx]

    return {
        "concise_answer": best_sentence,
        "source_chunk": chunks[source_chunk_index],
        "chunk_index": int(source_chunk_index),
        "sentence_score": round(float(sentence_scores[best_sent_idx]), 4),
        "chunk_score": round(float(chunk_scores[source_chunk_index]), 4),
        "total_chunks": len(chunks),
    }

# Run Questions and Display Results
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
questions = [
    "ŸÖÿ™Ÿâ ÿ™ŸÖ ÿ™ÿ±ŸÖŸäŸÖ ÿßŸÑŸÇŸÑÿπÿ© ÿßŸÑÿ®Ÿäÿ∂ÿßÿ°ÿü",
    "ŸÖŸÜ ÿßŸÑÿ∞Ÿä ÿ£ŸÖÿ± ÿ®ÿ®ŸÜÿßÿ° ÿßŸÑŸÇŸÑÿπÿ© ÿßŸÑÿ®Ÿäÿ∂ÿßÿ°ÿü",
    "ŸÖÿß ŸáŸà ÿßŸÑÿ∑ÿ±ÿßÿ≤ ÿßŸÑŸÖÿπŸÖÿßÿ±Ÿä ŸÑŸÑŸÇŸÑÿπÿ©ÿü",
    "ŸÉŸäŸÅ ÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸÇŸÑÿπÿ© ŸÅŸä ÿßŸÑÿπŸáÿØ ÿßŸÑÿπÿ´ŸÖÿßŸÜŸäÿü"
]

print("\n" + "=" * 60)
print("        üîç RAG SYSTEM RESULTS ‚Äî Arabic Castle Data")
print("=" * 60 + "\n")

for i, question in enumerate(questions, 1):
    print(f"‚ùì Question {i}: {question}")
    print("-" * 50)

    result = ask_question(question, chunks, chunk_embeddings, model)

    if result is None:
        print("‚ö†Ô∏è No answer found.")
    else:
        # ‚îÄ‚îÄ Primary output: concise answer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        print(f"‚úÖ Answer:\n{result['concise_answer']}")
        print()

        # ‚îÄ‚îÄ Secondary output: source chunk ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        print(f"üìÑ Source Chunk (index {result['chunk_index']}):\n{result['source_chunk']}")
        print()

        # ‚îÄ‚îÄ Metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        print(f"üìä Total chunks in document : {result['total_chunks']}")
        print(f"üéØ Sentence confidence score: {result['sentence_score']}")
        print(f"üì¶ Chunk confidence score   : {result['chunk_score']}")

    print("\n" + "=" * 60 + "\n")

!jupyter nbconvert --to script /content/*.ipynb

from google.colab import files
files.download("/content/arabic_rag.py")